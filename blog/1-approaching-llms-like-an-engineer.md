### Approaching LLMs Like an Engineer

We are approaching LLMs wrong. The industry is rushing to 'vibe code' results using massive context windows and brute-force agentic loops. This is the antithesis of software engineering. We are skipping primitives and jumping straight to leaky abstractions. To build better systems, we must stop treating LLMs as magic wands and start treating them as predictable, probabilistic components whose value becomes apparent when harnessed by a deterministic architecture.

This repository serves as a sandbox to anchor these views. The codebase demonstrates how to generate custom, personalized web components for key clients of a mock online ski shop by calling LLMs programatically. Notably, while almost every line of code here was written by an LLM, none of it was "vibe coded" or generated by autonomous agents. It was built using the precise interaction patterns detailed below. See the code in action: run `npm start:ski-shop`, navigate to `localhost:3000/admin.html` and check out the personalized LLM-generated views for yourself.

This is a living artifact. You will find holes. Hopefully, you will also find an invitation to [contribute](../CONTRIBUTING.md). Let's patch the holes together and advance our collective understanding.

## Pragmatic Principles For Better LLM Results

A large language model takes in tokens and returns a result representative of higher-order reasoning. The quality of this result is dependent on its training and size. There is a max number of input tokens that an LLM can consider in determining its response, referred to as the context window.

That's it for the academic overview. Here are some key principles behind interacting with LLMs to write code that I've found useful, which often defy conventional wisdom:

- **Context Hygiene**
- **Attention Dilution**
- **Deterministic Guardrails for Probabilistic Compute**
- **The Recursion of Technical Debt**

# Context Hygiene

LLMs operate on probability, not logic. The higher the ratio of 'irrelevant tokens' to 'domain-critical tokens' in your context window, the higher the entropy of the response. Vibe coding floods the context with noise (configuration, redundant documentation, framework boilerplate, etc.). By contrast, engineering with LLMs means carefully considering the state of your context window and how to improve (or reset) it at every step. Apply context hygiene to reduce the noise and improve the signal of each LLM interaction. A concise and semantic prompt beats an unfocused and overly verbose one every time.

Anthropic has published excellent research demonstrating the correlation between input quality and output quality: [Anthropic Research](https://www.anthropic.com/research/economic-index-primitives). I will summarize my observations more simply. The common phrase "garbage in, garbage out" is too passive. The reality is more like **"Input Fidelity -> Output Fidelity."** If you provide the model with low-resolution context, you cannot expect a high-resolution logical output.

# Attention Dilution

There is an inverse relationship between **Context Window Utilization** and **Reasoning Quality**. As you add more context, the model's ability to perform multi-step logic degrades non-linearly, especially if adding references to new and complex topics rather than incremental and linear additions.

Naive usage of LLMs may try to solve this by stuffing more context in. A better engineering approach is to shard the context: break the problem into atomic units that fit comfortably within the model's 'peak attention' window (usually the first 20-30% of its capacity, depending on the rigor of your context hygiene). This aligns with findings from Anthropic on [MCP Execution Quality](https://www.anthropic.com/engineering/code-execution-with-mcp), though I am taking liberties to extend these observations based on my interactions with the latest models.

# Deterministic Guardrails for Probabilistic Compute

We can validate probabilistic output deterministically. When an LLM 'hallucinates' code logic, it almost always degrades in code style first. In my experiments, a degradation in code comment quality is a leading indicator of potential faulty logic. Meaning, you don't need to run the code to know it's broken; you just need to mentally lint the LLM output. If your mental linter raises alarms, then discard the 'thought process' entirely and try again with better context. This is true for other LLM applications that demand higher order reasoning, not just code. You can easily reproduce this phenomenon by sampling a base model with a variety of simple reasoning questions against domains well represented in its training, then observing how its responses drift.

Consider the CAP theorem in cloud computing, which mathematically dictates that no distributed system can simultaneously provide Consistency, Availability, and Partition Tolerance. Did the existence of the CAP theorem force us back to mainframes? No. It forced us to carefully design systems that mitigate for the chosen tradeoff, in a way that is completely imperceptible to the end user.

Similarly, perhaps we must treat LLMs as systems with **"Eventual Accuracy."** We don't trust the probabilistic output blindly, so we carefully apply deterministic checks (the "mental linter") as our guardrail. Google's infrastructure around Gemini 3 Pro seems to handle confidence thresholds exceptionally well, to the point where it's hard to get it to write code that doesn't work. This forces the user to debug their prompt (the input) rather than wasting cycles debugging the hallucination (the output).

See [examples](#gemini-3-pro-chat-examples) of using Gemini 3 Pro to develop this codebase at the bottom of this post.

# The Recursion of Technical Debt

Agentic loops that are not carefully controlled risk generating **'Write-Only Code.'** This code may work for the immediate task at hand, but it lacks the 'cognitive compression' that a human (or a directed LLM) applies. When you ask an agent to 'fix X,' it is likely to subtly bypass how your domain is abstracted within your code by patching the problem with raw logic. Then it suggests more fixes, many of which may not be strictly relevant, further compounding the problem. Do this 10 times, and your codebase becomes a fractal of patches that no human, or context window, can understand.

There is undeniable value in agentic AI when applied carefully, particularly in enterprise settings where scope and process are more rigorously controlled. However, for the individual engineer, over-reliance on agents often outsources the learning process. If you lean on agents to bypass understanding, you aren't building a system; you're building a dependency. Carefully consider your goals before letting an autonomous loop dictate your architecture.

## Gemini 3 Pro Chat Examples

All code in this repository was written through precisely targeted interactions with Gemini 3 Pro, as inspired by the principles above. Examples of real code generating interactions below.

# Example: Unexpected Core Refactor

I set out to refactor one of the most important domain files in this project with a very specific idea of how I wanted its new logic to look. This likely caused me to make my initial prompt much more verbose than my typical prompts. This initial prompt seemingly tripped Gemini's confidence threshold, as it returned some internal gibberish, which was quickly replaced by a generic confidence error shown below.

[Gemini Confidence Error](./images/gemini-ex-1.png)

I hypothesized that this may be caused by a logic error in my prompt, as it's very strange for Gemini to return this response to an initial prompt.

[Prompt Logic Error](./images/gemini-ex-2.png)

I replaced a few words in the prompt to fix the error in prompt logic, as shown above, and Gemini produced the beautiful Javascript code that you see committed here (TODO: link to commit).

However, whatever happened in the first prompt may have the context window to go haywire. My next prompt was a simple one to get it to update the method arguments to match my actual input data structure. I have executed similar prompt as first or second shot many times with no issuess. For some reason, Gemini completely misinterpreted what I was trying to do this time, and instead asked me to link by Google Workspace.

[Gemini Unable to Parse Input](./images/gemini-ex-3.png)

I tried to be clever and update the prompt again by removing the word 'real' that I thought. Gemini was not able to make my desired change. Instead, it updated the code comments of the file. You can see the full chat detailed above here: https://gemini.google.com/share/1a1fdb177658.

[One Shot With New Context Window](./images/gemini-ex-5.png)

Are you skeptical of my hypotheses behind Gemini's behaviour? Above is a one-shot prompt that I just did ([chat here](https://gemini.google.com/share/e6ffc9d02a42)) where it produced beautiful Javascript code when given basically the exact same input that references the memoized file as the two failed attempts shown above. I had one shotted Gemini many times using the same prompt structure that references hard coded data, yet this was the only context window that this approach has ever failed in (and once in a spectacularly wrong way).

[Better Initial Prompt](./images/gemini-ex-4.png)

I also re-wrote my original prompt in a way that's more likely to work in one-shot in every time, shown above. It's still not perfect, but much better.

Still skeptical? I encourage you to take my word for nothing, and follow my workflow to try reproducing my results yourself, or better yet try to improve upon them. Checkout the commit `TODO: add commit`, run the script `npm run package-code`, import the full `tmp-llm-input` folder as code to a new Gemini 3 Pro chat, and prompt away! Feel free to start with my "better" initial prompt from the image above pasted here:

```
Here's a project I'm working on to show the power of leveraging LLMs to generate end-user facing web components.

I'd like to do a major refactor of the './terminal-value/generateValue.js' file. This is a breaking change so make changes in a new file called generateValueRefactor.js.

Make one internal method that returns a set of prompt generating functions. These functions should a basePrompt generator, which contains the 'businessStrategy' variable. Make it call a base function called generateBasePrompt that accepts all arguments required to generate this. It should also have a list of subprompt generators, which is defined in the 'getFooter' in the existing file. The subprompt generators should include one for homePage and one for orderPage that are implemented. Also include one for redditPost and twitterPost that are no-ops for now using the same pattern.

There should be a method exposed by this new file that is called 'generatePrompts'. It accepts a two arguments: a set that represent prompt types to generate and client details. For example ['order', 'home', 'reddit'] or just ['order'] is argument 1, and argument 2 is any client context required to generate those prompts.

Reference the './terminalValue/memoizedResults/parseValueResults.js' file to see input structure. Make everything pure and testable with all inputs being passed between functions and no global variables.
```

Don't try just once, try 20 times with different context windows, and cross your fingers that your request is hitting the same version of Gemini's background systems that mine did. My only ask is, if you produce a better result, please [contribute](../CONTRIBUTING.md) it back here.

Hopefully this post, or another like it, inspires you to unplug from the vibe coding noise on social media and start approaching LLMs like an engineer -- like predictable systems whose value is ripe to be harnessed by applying the right software engineering principles.

# Additional Examples

Update input of refactored generateValue.js file to match actual data structure (next commit after original example). This is a good example of what I mean by memoization or mocks, though by this point I got lazy and just pasted it into the file.
https://gemini.google.com/share/1a1fdb177658
TODO: Add commit link

Refactor of a major data structure input.
https://gemini.google.com/share/e394d1e6c1a2
TODO: Add commit link

Refactor of the core service method.
https://gemini.google.com/share/eb623065066f
TODO: Add commit link

Create the helper script used in root to interact with LLM (previously, I was manually pasting)
https://gemini.google.com/share/71041e15531e
TODO: Add commit link

Feature to generate web component files that are served to users from prompts
https://gemini.google.com/share/7fa5ed003d78
TODO: Add commit link

Feature to generate prompts from the operational DB state
https://gemini.google.com/share/e63f086ac0f8
TODO: Add commit link

Multiple complex front-end features being generated in one-shot in a single chat
https://gemini.google.com/share/816018a113b9
TODO: Add commit link

### Approaching LLMs Like an Engineer

I see a problem with how we are approaching LLMs. We are rushing to produce results through "vibe coding" with massive context windows and brute-force agentic loops. This is the antithesis of software engineering. We are skipping the primitives and jumping straight to the heavy abstractions. This post is an argument for a return to basics. Let's treat LLMs as controlled components within our tech ecosystem, rather than magic wands.

This repository serves as a sandbox to anchor these views. The codebase demonstrates how to generate custom, personalized web components for a mock ski shop using LLMs. Notably, while almost every line of code here was written by an LLM, none of it was "vibe coded" or generated by autonomous agents. It was built using precise, deterministic interaction patterns detailed below. See the code in action: clone the repo, install packges, run `npm start:ski-shop`, and navigate to `localhost:3000/admin.html`.

This is a living artifact. You will find holes. Hopefully, you will also find an invitation to contribute. Let's patch the holes together and advance our collective understanding.

## Pragmatic Principles For Better LLM Results

A large language model takes in tokens and returns a result representative of higher-order reasoning. The quality of this result is dependent on its training and size. There is a max number of input tokens that an LLM can consider in determining its response, referred to as the context window.

That's it for the academic overview. Here are the key principles behind interacting with LLMs that I've observed, which often defy conventional wisdom:

- **Context Hygiene**
- **Attention Dilution**
- **Deterministic Guardrails for Probabilistic Compute**
- **The Recursion of Technical Debt**

# Context Hygiene

LLMs operate on probability, not logic. The higher the ratio of 'irrelevant tokens' to 'domain-critical tokens' in your context window, the higher the entropy of the response. Vibe coding floods the context with noise (framework boilerplate, entire files). By contrast, engineering with LLMs means aggressively curating 'Context Hygiene' -- stripping documentation, minimizing noise, and passing only the exact signatures needed. A concise and semantic prompt beats an unfocused and overly verbose one every time.

Anthropic has published excellent research demonstrating the correlation between input quality and output quality: [Anthropic Research](https://www.anthropic.com/research/economic-index-primitives). I will summarize my observations more simply: The phrase "garbage in, garbage out" is too passive. The reality is more like **"Input Fidelity -> Output Fidelity."** If you provide the model with low-resolution context, you cannot expect high-resolution logic.

# Attention Dilution

There is an inverse relationship between **Context Window Utilization** and **Reasoning Quality**. As you add more context, the model's ability to perform multi-step logic degrades non-linearly, especially if adding references to new, complex topics rather than new incremental, linear context.

Naive 'Agentic' loops often try to solve this by stuffing more context in. The engineering approach is to shard the context: break the problem into atomic units that fit comfortably within the model's 'peak attention' window (usually the first 20-30% of its capacity, depending on the rigor of your context hygiene). This aligns with findings from Anthropic on [MCP Execution Quality](https://www.anthropic.com/engineering/code-execution-with-mcp), though I am extending these observations based on my interactions with the latest models.

# Deterministic Guardrails for Probabilistic Compute

We can validate probabilistic output deterministically. When an LLM 'hallucinates' logic, it almost always 'hallucinates' syntax first. In my experiments, a degradation in code comment quality is a leading indicator of a logic bug. Meaning, you don't need to run the code to know it's broken; you just need to mentally lint the LLM output. If your mental linter raises alarms, then discard the 'thought process' entirely and try again with better context. This is true for other LLM applications that demand higher order reasoning, not just code. You can easily reproduce this by using a base model and sampling it with a variety of simple reasoning questions against domains well represented in its training.

Consider the CAP theorem in distributed systems, which dictates that no system can simultaneously provide Consistency, Availability, and Partition Tolerance. Did the existence of the CAP theorem force us back to mainframes? No. It forced us to carefully design systems that mitigate it, in a way that is completely imperceptible to the end user.

Similarly, perhaps we must treat LLMs as systems with "Eventual Accuracy." We don't trust the probabilistic output blindly; we apply deterministic checks (the "mental linter") as our guardrail. Gemini 3 Pro's ability to enforce these deterministic guardrails is a leap ahead of previous LLM-based products I have used. See examples of using it to write code at the bottom of this post.

# The Recursion of Technical Debt

Agentic loops risk generating 'Write-Only Code.' It may work for the immediate task at hand, but it lacks the 'cognitive compression' that a human (or a directed LLM) applies. When you ask an agent to 'fix X,' it often bypasses the abstraction you carefully built and patches the raw logic, which dilutes your actual domain code. Then it suggests more fixes, many of which may not be relevant, further compounding the problem. Do this 10 times, and your codebase becomes a fractal of patches that no human, or context window, can understand.

There is undeniable value in agentic AI when applied carefully, particularly in enterprise contexts where scope and process must be rigidly defined. However, for individual engineering, it often outsources the learning process. If you lean on agents to bypass understanding, you aren't building a system; you're building a dependency. Carefully consider your goals before letting an autonomous loop dictate your architecture.

## Examples From my Personal LLM Usage

All the code you see here to date was written entirely by chatting manually (copy / pasting) with Gemini 3 Pro over the course of a few days. I got better at it over time, but I applied the same principles from the beginning.

I never considered my interaction to be a chat, instead I always tried to carefully consider what context window Gemini was working with and what I was trying to do. I also never vibe coded. I spent a bunch of time in debugger breakpoints analyzing and copying (later memoizing) large data structures created by my app or returned by the Gemini Batch APIs so both the LLM and I could understand them. At the start of each session, I spent some time ramping up my own mental context as to what I wanted to do next and where I left off while sipping coffee and lightly cleaning up code.

The point I'm trying to make, is that this wasn't vibe coding, but it also wasn't agentic AI either. Do we have a term that describes using LLMs as a precise tool piloted by experts to amplify (not just reproduce) results? Maybe this would be a good place to start.

# Applied Principles Behind This Codebase

There are a handful of things I did intentionally while developing that worked out well.

- No markdown files or unnecessary formatting (such as headers) passed to LLM.

Instead, I focused on keeping my code as semantic as possible, and gave the LLM only brief handwritten prompts that referenced certain files or methods along with concise higher order instructions.

- No files unrelated to my domain such as configuration.

There is a utility file called `packageCodeForLlm` at root of this codebase. My workflow involved running this script to strip all irrelevant or dangerous context before uploading all project files to the LLM in my initial prompt.

- Limited use of libraries.

This was partly to experiment, but the results were surprisingly good. I only used stuff as ubiquitous as a web server to avoid hidden context. I did import the Gemini SDK, which was a mistake in hindsight as manually constructing requests would have been more effective.

- Write pure code that supports memoized datastructures.

I developed functionality in chunks generally running an orchestration method and saving the results of it to a file either manually or later with memoization. The LLM was able to flawlessly handle any external context with this approach. It's also generally good practice when working with an interpreted language like JS.

Side note: I have seen some industry benchmarks of LLMs ability to produce code in various languages. Generally, interpreted languages score much lower than compiled languages. I can guarantee that these benchmarks are not taking advantage of the real benefit of using an interpreted language, where something like memoization is trivial to do.

- Ask Gemini to produce full file responses to all my prompts.

I started with more of a bottom-up approach and focused on methods. LLMs are not a bottom-up tool. Having Gemini produce full files, even if I was making a tiny change to one method in the file, always worked better than me having to wrangle individual methods or lines of code. One thing I was careful of was asking it to make these small changes with too big of a context window loaded, as this would increase chance of random refactoring rather than precise outputs.

- Consider what context LLM has carefully in each interaction.

I would stick with one chat for efficiency if the context was still relevant, or if my request for the LLM was trivial enough for it to not matter. As soon as I jumped to a new feature requiring different higher order context, or if my chat got too long, I would create a new oen and start over again.

## Gemini 3 Pro Chat Examples

NOTE: all code input in examples is my full project stripped of irrelevant context. By the last prompts, this was ~95 files. Always manually uploaded, never linked to a repo.

# Refactor Core Prompt Generation Logic

https://gemini.google.com/share/1a1fdb177658
TODO: Add commit link (after uploading)

My initial one-shot prompt was probably too verbose, but this file is so critical to the project that I described it more than I did others.

What's really interesting is that my first attempt tripped Gemini's external confidence meter and it returned `Sorry, something went wrong. Please try your request again.` after flashing some internal method call gibberish. I have only been able to get this a few times after longer chats previously.

Previous times I got this, re-running the prompt returned the same result, so I analyzed my prompt for any logic input errors and updated exactly two words from the input: `I'd like the new file to have two methods.` -> `I'd like the new file to have the following methods.`

As you can see, it then did a beautiful job of refactoring the file.

What's also really interesting is how Gemini responded to the rest of the chat. I had a few other simple things I wanted it to do to this file, namely refactor the input data structure (next prompt). It performed so badly on the next two prompts, completely missing context behind what would have likely been easy one-shots with a fresh context.

# Refactor Prompt Generation Data Structure Input

https://gemini.google.com/share/e394d1e6c1a2
TODO: Add commit link

A clean example of what I mean by memoization and the advantages of interpreted languages with LLMs. Gemini did a great job refactoring in the previous example, but it made up the data structure of the input. A simple example object and 3 sentences resulted in beautiful Javascript code. I was getting lazy by this point, this was my last commit of the day, so I just pasted a reference into the file itself.

# Refactor Core Service

https://gemini.google.com/share/eb623065066f
TODO: Add commit link

# Create Helper Script to Upload Files to LLM

https://gemini.google.com/share/71041e15531e
TODO: Add commit link

# Feature: Serve Web Components from Prompt Responses

https://gemini.google.com/share/7fa5ed003d78
TODO: Add commit link

# Feature: Generate Prompts from DB State

https://gemini.google.com/share/e63f086ac0f8
TODO: Add commit link

# Multiple Complex Front-end Features in One Chat

https://gemini.google.com/share/816018a113b9
TODO: Add commit link

--

## An Analogy For Developers

To make the example more concrete, let's consider the journey of learning web development to solve problems, as analogous to learning how to use LLMs to solve problems.

There is an overwhelming wealth of information available to a new web developer. It's possible to begin your learning journey at the leading edge of abstractions that top industry firms are using. You may even be able to important a few open source programs and get them to run. Does this act now make you an industry-leading web developer?

The journey of becoming a web developer looks more like this: learn basics of HTTP and web / server communication, learn markup, learn languages, learn runtime environments, etc. Once you have that foundation, then you may choose to specialize, and after repeating the cycle a few times, then you may become that industry-leading expert in your area.

Now ask yourself, is this how you are approaching learning about LLMs? How much time have you spent experimenting with the basic building blocks vs. cloning the latest Enterprise-grade agentic toolchain?

The purpose of this blog post is to help guide you towards a foundation of what core concepts may be of applying LLMs to solve real-world problems, just like we do in any other paradigm at our disposal in the software engineering world.

You can see this post is embedded in a code repository. This will serve as my foundation to share examples (commits and LLM interactions) that back up my points, reflective of my own experience. This project itself is doing some cool stuff around using LLMs to generate code. This is fully working but not easy to grok or run. I will write about this in future posts here as it evolves, if there's something worth writing about.

## Defining a "Large Language Model"

There are a few progressive principles that we'll walk through. The most important one is bifurcating the definition of "Large Language Model".

- Base Large Language Models: a commmodity software tool that takes in tokens and produces output reflective of higher order reasoning, if the tokens match its training data well enough. A context window is simply how many tokens the LLM can reason about at any given time. A chat is simply tokens fed back into a context window in a clever way to simulate an ongoing engagement. Examples: Qwen, Llama, phi, etc.
- Large Language Model-based Produts: complex layers of distributed systems made up LLMs and other traditional servers applied in many different contexts, as well as more traditional software tools. Examples: Gemini, ChatGPT, Claude.

The first one is a commodity tool. The second one is a multi-billion dollar software system. Have you ever interacted with the first one directly? And a bonus question, do you have the word "AI Product Expert" or "AI Engineer" in your bio? If your answers are no and yes, I don't blame you, but please consider how this may negatively impact your learning over time.

## Experiments For Engineers

I believe that technologists would benefit more from experimenting with base LLMs directly, models with no magic under the hood, rather than LLM-based products that most are using. Consider what LLM-bassed products are actually doing.

# The Magic Behind Multi-billion Dollar LLM Products

Claude, Gemini and ChatGPT are examples of products built on top of LLMs worth billions of dollars. When you submit a query to a multi-billion dollar product like this, the actual behind the scenes is very different from how LLMs behave at a baseline. At a high-level, you can imagine the request processing flows of these products to be something more like this:

- Parse Input (determine what type of response the user is looking for, and extract key pieces of domain information)
- Inject Additional Context (summarize some other external knowledge repository to add context to the prompt)
- Query Internal LLM With Enriched Context (input the user's query to an internal LLM with enriched user context, plus additional internal magical context not visible to us)
- Verify External Confidence (this can be multiple forms of confidence, such as is the user doing something malicious, or is the answer correct so far)
- Summarize Result (this simulates a "thought process" as results are summarized in chunks and displayed back in the front-end)
- Repeat (ad nauseum)

I'm not going to spend much more time on this because I don't have any actual visibility into how these billion-dollar products function beyond my surface insight. The point is to illustrate that there is so much magic behind the scenes here that it is not a good way to build a foundation of learning, which is what will actually help you grow and better interact with these machines (or even build your own!) in the future.

# Suggested Experiments

Spin up some of your favourite base LLMs with default settings. I recommend Qwen, Llama and phi as a starting point. Here are some experiments you can do to start building a foundation of context and experiential knowledge with LLMs.

After each experiment, note down your observations. What does the distribution of results? How is this different from past results? Can you start start to see or predict any future trends?

- Execute the same one-shot prompt many times in a row against the same model.
- Execute this prompt against other models, or models in the same family of different sizes.
- Create a few simple higher level reasoning tasks that are difficult to solve with traditional programming techniques (perhaps look to open source benchmarks for inspiration). Execute those many times against a variety of models.
- Execute prompts with multiple shots by feeding the results of earlier prompts back into the LLM. Run the same series of prompts many times in a row as well with different models.
- Play around with the size of the context window and execute the same one-shot prompts while the LLM has varied context loaded up.
- Try to trick the LLM in as many different ways as you can, subtle or not so subtle. Play around with adding references to irrelevant domain concepts or objects, like a layperson might, and run these prompts many times against a variety of LLMs.
- Summarize the results of your chain of prompts with another LLM to replicate the "thinking" loading bar that LLM-products show.

The only tricky thing about running these experiments is getting access to base models larger than a certain size due to hardware requirements. If anyone would like to collaborate on setting this up, I have many ideas for a learning sandbox that I'd love to contribute.

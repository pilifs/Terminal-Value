### Approaching LLMs Like an Engineer

I see a problem with how we are approaching LLMs. We are rushing to produce results through "vibe coding" with massive context windows and brute-force agentic loops. This is the antithesis of software engineering. We are skipping the primitives and jumping straight to the heavy abstractions. This post is an argument for a return to basics. Let's treat LLMs as controlled components within our tech ecosystem, rather than magic wands.

This repository serves as a sandbox to anchor these views. The codebase demonstrates how to generate custom, personalized web components for a mock ski shop using LLMs. Notably, while almost every line of code here was written by an LLM, none of it was "vibe coded" or generated by autonomous agents. It was built using precise, deterministic interaction patterns detailed below. See the code in action: clone the repo, install packges, run `npm start:ski-shop`, and navigate to `localhost:3000/admin.html`.

This is a living artifact. You will find holes. Hopefully, you will also find an invitation to contribute. Let's patch the holes together and advance our collective understanding.

## Pragmatic Principles For Better LLM Results

A large language model takes in tokens and returns a result representative of higher-order reasoning. The quality of this result is dependent on its training and size. There is a max number of input tokens that an LLM can consider in determining its response, referred to as the context window.

That's it for the academic overview. Here are the key principles behind interacting with LLMs that I've observed, which often defy conventional wisdom:

- **Context Hygiene**
- **Attention Dilution**
- **Deterministic Guardrails for Probabilistic Compute**
- **The Recursion of Technical Debt**

# Context Hygiene

LLMs operate on probability, not logic. The higher the ratio of 'irrelevant tokens' to 'domain-critical tokens' in your context window, the higher the entropy of the response. Vibe coding floods the context with noise (framework boilerplate, entire files). By contrast, engineering with LLMs means aggressively curating 'Context Hygiene' -- stripping documentation, minimizing noise, and passing only the exact signatures needed. A concise and semantic prompt beats an unfocused and overly verbose one every time.

Anthropic has published excellent research demonstrating the correlation between input quality and output quality: [Anthropic Research](https://www.anthropic.com/research/economic-index-primitives). I will summarize my observations more simply: The phrase "garbage in, garbage out" is too passive. The reality is more like **"Input Fidelity -> Output Fidelity."** If you provide the model with low-resolution context, you cannot expect high-resolution logic.

# Attention Dilution

There is an inverse relationship between **Context Window Utilization** and **Reasoning Quality**. As you add more context, the model's ability to perform multi-step logic degrades non-linearly, especially if adding references to new, complex topics rather than new incremental, linear context.

Naive 'Agentic' loops often try to solve this by stuffing more context in. The engineering approach is to shard the context: break the problem into atomic units that fit comfortably within the model's 'peak attention' window (usually the first 20-30% of its capacity, depending on the rigor of your context hygiene). This aligns with findings from Anthropic on [MCP Execution Quality](https://www.anthropic.com/engineering/code-execution-with-mcp), though I am extending these observations based on my interactions with the latest models.

# Deterministic Guardrails for Probabilistic Compute

We can validate probabilistic output deterministically. When an LLM 'hallucinates' logic, it almost always 'hallucinates' syntax first. In my experiments, a degradation in code comment quality is a leading indicator of a logic bug. Meaning, you don't need to run the code to know it's broken; you just need to mentally lint the LLM output. If your mental linter raises alarms, then discard the 'thought process' entirely and try again with better context. This is true for other LLM applications that demand higher order reasoning, not just code. You can easily reproduce this by using a base model and sampling it with a variety of simple reasoning questions against domains well represented in its training.

Consider the CAP theorem in distributed systems, which dictates that no system can simultaneously provide Consistency, Availability, and Partition Tolerance. Did the existence of the CAP theorem force us back to mainframes? No. It forced us to carefully design systems that mitigate it, in a way that is completely imperceptible to the end user.

Similarly, perhaps we must treat LLMs as systems with "Eventual Accuracy." We don't trust the probabilistic output blindly; we apply deterministic checks (the "mental linter") as our guardrail. Gemini 3 Pro's ability to enforce these deterministic guardrails is a leap ahead of previous LLM-based products I have used. See examples of using it to write code at the bottom of this post.

# The Recursion of Technical Debt

Agentic loops risk generating 'Write-Only Code.' It may work for the immediate task at hand, but it lacks the 'cognitive compression' that a human (or a directed LLM) applies. When you ask an agent to 'fix X,' it often bypasses the abstraction you carefully built and patches the raw logic, which dilutes your actual domain code. Then it suggests more fixes, many of which may not be relevant, further compounding the problem. Do this 10 times, and your codebase becomes a fractal of patches that no human, or context window, can understand.

There is undeniable value in agentic AI when applied carefully, particularly in enterprise contexts where scope and process must be rigidly defined. However, for individual engineering, it often outsources the learning process. If you lean on agents to bypass understanding, you aren't building a system; you're building a dependency. Carefully consider your goals before letting an autonomous loop dictate your architecture.

## Examples From my Personal LLM Usage

All the code you see here to date was written entirely by chatting manually (copy / pasting) with Gemini 3 Pro over the course of a few days. I got better at it over time, but I applied the same principles from the beginning.

I never considered my interaction to be a chat, instead I always tried to carefully consider what context window Gemini was working with and what I was trying to do. I also never vibe coded. I spent a bunch of time in debugger breakpoints analyzing and copying (later memoizing) large data structures created by my app or returned by the Gemini Batch APIs so both the LLM and I could understand them. At the start of each session, I spent some time ramping up my own mental context as to what I wanted to do next and where I left off while sipping coffee and lightly cleaning up code.

The point I'm trying to make, is that this wasn't vibe coding, but it also wasn't agentic AI either. Do we have a term that describes using LLMs as a precise tool piloted by experts to amplify (not just reproduce) results? Maybe this would be a good place to start.

# Applied Principles Behind This Codebase

There are a handful of things I did intentionally while developing that worked out well.

- No markdown files or unnecessary formatting (such as headers) passed to LLM.

Instead, I focused on keeping my code as semantic as possible, and gave the LLM only brief handwritten prompts that referenced certain files or methods along with concise higher order instructions.

- No files unrelated to my domain such as configuration.

There is a utility file called `packageCodeForLlm` at root of this codebase. My workflow involved running this script to strip all irrelevant or dangerous context before uploading all project files to the LLM in my initial prompt.

- Limited use of libraries.

This was partly to experiment, but the results were surprisingly good. I only used stuff as ubiquitous as a web server to avoid hidden context. I did import the Gemini SDK, which was a mistake in hindsight as manually constructing requests would have been more effective.

- Write pure code that supports memoized datastructures.

I developed functionality in chunks generally running an orchestration method and saving the results of it to a file either manually or later with memoization. The LLM was able to flawlessly handle any external context with this approach. It's also generally good practice when working with an interpreted language like JS.

Side note: I have seen some industry benchmarks of LLMs ability to produce code in various languages. Generally, interpreted languages score much lower than compiled languages. I can guarantee that these benchmarks are not taking advantage of the real benefit of using an interpreted language, where something like memoization is trivial to do.

- Ask Gemini to produce full file responses to all my prompts.

I started with more of a bottom-up approach and focused on methods. LLMs are not a bottom-up tool. Having Gemini produce full files, even if I was making a tiny change to one method in the file, always worked better than me having to wrangle individual methods or lines of code. One thing I was careful of was asking it to make these small changes with too big of a context window loaded, as this would increase chance of random refactoring rather than precise outputs.

- Consider what context LLM has carefully in each interaction.

I would stick with one chat for efficiency if the context was still relevant, or if my request for the LLM was trivial enough for it to not matter. As soon as I jumped to a new feature requiring different higher order context, or if my chat got too long, I would create a new oen and start over again.

## Gemini 3 Pro Chat Examples

NOTE: all code input in examples is my full project stripped of irrelevant context. By the last prompts, this was ~95 files. Always manually uploaded, never linked to a repo.

# Refactor Core Prompt Generation Logic

https://gemini.google.com/share/1a1fdb177658
TODO: Add commit link (after uploading)

My initial one-shot prompt was probably too verbose, but this file is so critical to the project that I described it more than I did others.

What's really interesting is that my first attempt tripped Gemini's external confidence meter and it returned `Sorry, something went wrong. Please try your request again.` after flashing some internal method call gibberish. I have only been able to get this a few times after longer chats previously.

Previous times I got this, re-running the prompt returned the same result, so I analyzed my prompt for any logic input errors and updated exactly two words from the input: `I'd like the new file to have two methods.` -> `I'd like the new file to have the following methods.`

As you can see, it then did a beautiful job of refactoring the file.

What's also really interesting is how Gemini responded to the rest of the chat. I had a few other simple things I wanted it to do to this file, namely refactor the input data structure (next prompt). It performed so badly on the next two prompts, completely missing context behind what would have likely been easy one-shots with a fresh context.

# Refactor Prompt Generation Data Structure Input

https://gemini.google.com/share/e394d1e6c1a2
TODO: Add commit link

A clean example of what I mean by memoization and the advantages of interpreted languages with LLMs. Gemini did a great job refactoring in the previous example, but it made up the data structure of the input. A simple example object and 3 sentences resulted in beautiful Javascript code. I was getting lazy by this point, this was my last commit of the day, so I just pasted a reference into the file itself.

# Refactor Core Service

https://gemini.google.com/share/eb623065066f
TODO: Add commit link

# Create Helper Script to Upload Files to LLM

https://gemini.google.com/share/71041e15531e
TODO: Add commit link

# Feature: Serve Web Components from Prompt Responses

https://gemini.google.com/share/7fa5ed003d78
TODO: Add commit link

# Feature: Generate Prompts from DB State

https://gemini.google.com/share/e63f086ac0f8
TODO: Add commit link

# Multiple Complex Front-end Features in One Chat

https://gemini.google.com/share/816018a113b9
TODO: Add commit link
